{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from skimage.color import rgb2lab, lab2rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CocoColorizationDataset(Dataset):\n",
    "    \"\"\"\n",
    "        L: [1, H, W]   ([0,1])\n",
    "        ab: [2, H, W]  ([-1,1])\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 root_dir=\"data/coco/train2017\", \n",
    "                 transform_size=128, \n",
    "                 limit=5000):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        valid_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "        self.files = [fname for fname in sorted(os.listdir(root_dir))\n",
    "                      if os.path.splitext(fname)[1].lower() in valid_extensions]\n",
    "        \n",
    "        self.files = self.files[:limit]\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((transform_size + 20, transform_size + 20)),\n",
    "            T.RandomCrop((transform_size, transform_size)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "        ])\n",
    "        \n",
    "        print(f\"Found {len(self.files)} valid images in {self.root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            fname = self.files[idx]\n",
    "            img_path = os.path.join(self.root_dir, fname)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = self.transform(img)\n",
    "            img_np = np.array(img).astype(np.float32) / 255.0\n",
    "            lab = rgb2lab(img_np)\n",
    "            L = lab[:, :, 0]    # L [0, 100]\n",
    "            ab = lab[:, :, 1:3] # ab [-128, 127]\n",
    "            L = L / 100.0      # [0, 1]\n",
    "            ab = ab / 128.0    # [-1, 1]\n",
    "            # [C, H, W]\n",
    "            L = torch.from_numpy(L).unsqueeze(0)           # [1, H, W]\n",
    "            ab = torch.from_numpy(ab).permute(2, 0, 1)       # [2, H, W]\n",
    "            return L, ab\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {fname}: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# (DoubleConv + MaxPool) => output (skip, pooled)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.double_conv(x)\n",
    "        x_down = self.pool(x)\n",
    "        return x, x_down\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        self.up_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(out_channels*2, out_channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        \"\"\"\n",
    "        x: features from previous layer\n",
    "        skip: with Down output(before pool)\n",
    "        \"\"\"\n",
    "        x = self.up_transpose(x)  # upsampling => [B, out_channels, H*2, W*2]\n",
    "        x = torch.cat([skip, x], dim=1)   # skip_channels + out_channels\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#    input: in_channels=4 (L 1 + clickmap 1 + color (a,b) 2)\n",
    "#    output: out_channels=2 (a, b)\n",
    "class UNetColorNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=2):\n",
    "        \"\"\"\n",
    "         down1: 4  -> 64\n",
    "         down2: 64 -> 128\n",
    "         down3: 128-> 256\n",
    "         down4: 256-> 512\n",
    "         bottleneck: 512->1024\n",
    "         up1: 1024->512 + skip(512) => conv=512\n",
    "         up2: 512->256 + skip(256) => conv=256\n",
    "         up3: 256->128 + skip(128) => conv=128\n",
    "         up4: 128->64  + skip(64)  => conv=64\n",
    "         final: 64 -> 2  (ab)\n",
    "        \"\"\"\n",
    "        super(UNetColorNet, self).__init__()\n",
    "        \n",
    "        self.down1 = Down(in_channels, 64)   # 4 -> 64\n",
    "        self.down2 = Down(64, 128)           # 64 -> 128\n",
    "        self.down3 = Down(128, 256)          # 128-> 256\n",
    "        self.down4 = Down(256, 512)          # 256-> 512\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up1 = Up(1024, 512)  # in=1024 => out=512; cat skip (512) conv 512\n",
    "        self.up2 = Up(512, 256)   # in=512 => out=256; cat skip (256) conv 256\n",
    "        self.up3 = Up(256, 128)   # in=256 => out=128; cat skip (128) conv 128\n",
    "        self.up4 = Up(128, 64)    # in=128 => out=64;  cat skip (64)  conv 64\n",
    "\n",
    "        # 64 => 2 (a, b)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        # Tanh [-1,1]\n",
    "        self.out_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, 4, H, W]\n",
    "        # ------------------- Down ---------------------\n",
    "        d1_skip, d1_down = self.down1(x)     # d1_skip: [B, 64, H, W] ; d1_down: [B, 64, H/2, W/2]\n",
    "        d2_skip, d2_down = self.down2(d1_down) # d2_skip: [B,128,H/2,W/2] ; d2_down: [B,128,H/4,W/4]\n",
    "        d3_skip, d3_down = self.down3(d2_down) # d3_skip: [B,256,H/4,W/4] ; d3_down: [B,256,H/8,W/8]\n",
    "        d4_skip, d4_down = self.down4(d3_down) # d4_skip: [B,512,H/8,W/8] ; d4_down: [B,512,H/16,W/16]\n",
    "        \n",
    "        # ------------------ Bottleneck ----------------\n",
    "        btm = self.bottleneck(d4_down)   # [B,1024,H/16,W/16]\n",
    "\n",
    "        # ------------------- Up -----------------------\n",
    "        u1 = self.up1(btm, d4_skip)      # [B,512,H/8, W/8]\n",
    "        u2 = self.up2(u1, d3_skip)       # [B,256,H/4, W/4]\n",
    "        u3 = self.up3(u2, d2_skip)       # [B,128,H/2, W/2]\n",
    "        u4 = self.up4(u3, d1_skip)       # [B,64, H,   W]\n",
    "\n",
    "        out = self.final_conv(u4)        # [B,2,H,W]\n",
    "        out = self.out_activation(out)   # [-1,1]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        \"\"\"\n",
    "          down1: 3   -> 64\n",
    "          down2: 64  -> 128\n",
    "          down3: 128 -> 256\n",
    "          down4: 256 -> 512\n",
    "          bottleneck: 512 -> 1024\n",
    "          up1: 1024 -> 512  + skip(512)  => conv=512\n",
    "          up2: 512  -> 256  + skip(256)  => conv=256\n",
    "          up3: 256  -> 128  + skip(128)  => conv=128\n",
    "          up4: 128  -> 64   + skip(64)   => conv=64\n",
    "          final: 64 -> 1 (clickmap)\n",
    "        \"\"\"\n",
    "        super(EditNet, self).__init__()\n",
    "        self.down1 = Down(in_channels, 64)    # 3 -> 64\n",
    "        self.down2 = Down(64, 128)            # 64 -> 128\n",
    "        self.down3 = Down(128, 256)           # 128 -> 256\n",
    "        self.down4 = Down(256, 512)           # 256 -> 512\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)  # Bottleneck\n",
    "\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.out_activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pred_ab, gray):\n",
    "        \"\"\"\n",
    "          pred_ab: [B, 2, H, W]\n",
    "          gray: [B, 1, H, W]\n",
    "          click_map: [B, 1, H, W]\n",
    "        \"\"\"\n",
    "        x = torch.cat([gray, pred_ab], dim=1)  # [B, 3, H, W]\n",
    "\n",
    "        d1_skip, d1_down = self.down1(x)  # d1_skip: [B,64,H,W]  ; d1_down: [B,64,H/2,W/2]\n",
    "        d2_skip, d2_down = self.down2(d1_down)  # [B,128,H/2,W/2] -> [B,128,H/4,W/4]\n",
    "        d3_skip, d3_down = self.down3(d2_down)  # [B,256,H/4,W/4] -> [B,256,H/8,W/8]\n",
    "        d4_skip, d4_down = self.down4(d3_down)  # [B,512,H/8,W/8] -> [B,512,H/16,W/16]\n",
    "\n",
    "        # Bottleneck\n",
    "        btm = self.bottleneck(d4_down)  # [B,1024,H/16,W/16]\n",
    "\n",
    "        u1 = self.up1(btm, d4_skip)   # [B,512,H/8,W/8]\n",
    "        u2 = self.up2(u1, d3_skip)    # [B,256,H/4,W/4]\n",
    "        u3 = self.up3(u2, d2_skip)    # [B,128,H/2,W/2]\n",
    "        u4 = self.up4(u3, d1_skip)    # [B,64,H,W]\n",
    "\n",
    "        out = self.final_conv(u4)     # [B,1,H,W]\n",
    "        out = self.out_activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def initial_clickmap(x_L, x_ab, num_clicks=50):\n",
    "    \"\"\"\n",
    "    random choose num_clicks pixel\n",
    "    set click position as 1 on clickmask\n",
    "    in ab channel, ab value of GT at postion of clickmask, other place 0 \n",
    "    \n",
    "    input:\n",
    "      x_L: [B, 1, H, W] L channel\n",
    "      x_ab: [B, 2, H, W] ab channel of GT\n",
    "    output:\n",
    "      clickmap: [B, 3, H, W] clickmask + ab\n",
    "    \"\"\"\n",
    "    B, _, H, W = x_L.shape\n",
    "    clickmap = torch.zeros(B, 3, H, W, device=x_L.device)\n",
    "    \n",
    "    for b in range(B):\n",
    "        num_pixels = H * W\n",
    "        indices = np.random.choice(num_pixels, num_clicks, replace=False)\n",
    "        # 1d => 2d\n",
    "        ys = indices // W\n",
    "        xs = indices % W\n",
    "        for y, x in zip(ys, xs):\n",
    "            clickmap[b, 0, y, x] = 1.0\n",
    "            clickmap[b, 1:, y, x] = x_ab[b, :, y, x]\n",
    "    return clickmap\n",
    "\n",
    "\n",
    "def update_clickmap(current_click, pred_click, x_ab, threshold=0.5):\n",
    "    \"\"\"\n",
    "    With EditNet update cumulative clickmap\n",
    "      current_click: [B,3,H,W] current cumulative clickmap (1 channel clickmask, 2 ab channel)\n",
    "      pred_click:  [B,1,H,W] EditNet predicted [0,1]\n",
    "      x_ab:        [B,2,H,W] GT ab \n",
    "    if pred_click > threshold => update\n",
    "    GT value in ab channel\n",
    "    \"\"\"\n",
    "    new_click = current_click.clone()\n",
    "    mask = ((new_click[:, 0:1, :, :] == 0) & (pred_click > threshold))\n",
    "    new_click[:, 0:1, :, :] = new_click[:, 0:1, :, :] + mask.float()\n",
    "    new_click[:, 1:3, :, :] = torch.where(mask.expand_as(x_ab), x_ab, new_click[:, 1:3, :, :])\n",
    "    return new_click\n",
    "\n",
    "\n",
    "def compute_psnr(pred, target):\n",
    "    \"\"\"\n",
    "    calculate PSNR of single image, both input pred and target at ab channel\n",
    "    shape [2, H, W], at [-1,1]\n",
    "    normalize to [0,1] and then calculate PSNR\n",
    "    \"\"\"\n",
    "    # ab [-1,1] => [0,1]\n",
    "    pred = (pred + 1) / 2\n",
    "    target = (target + 1) / 2\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    psnr = 10 * torch.log10(1.0 / mse)\n",
    "    return psnr.item()\n",
    "\n",
    "\n",
    "def train_one_epoch(colornet, editnet, dataloader, optimizer, device, \n",
    "                              criterion=nn.MSELoss(), num_iterations=4, initial_clicks=50, threshold=0.5):\n",
    "    colornet.train()\n",
    "    editnet.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (x_L, x_ab) in enumerate(dataloader):\n",
    "        x_L = x_L.to(device)   # [B,1,H,W]\n",
    "        x_ab = x_ab.to(device) # [B,2,H,W]\n",
    "        clickmap = initial_clickmap(x_L, x_ab, num_clicks=initial_clicks).to(device)  # [B,3,H,W]\n",
    "        cumulative_clickmap = clickmap.clone()\n",
    "        \n",
    "        loss_iter_total = 0.0\n",
    "        \n",
    "        colornet_input = torch.cat([x_L, cumulative_clickmap], dim=1)  # [B,4,H,W]\n",
    "        pred_ab = colornet(colornet_input)\n",
    "        loss_iter_total += criterion(pred_ab, x_ab)\n",
    "        \n",
    "        for i in range(1, num_iterations):\n",
    "            # EditNet input：predicted ab and clickmask\n",
    "            pred_click = editnet(pred_ab, x_L)  # [B,1,H,W], predict clickmask\n",
    "            # update cumulative clickmap\n",
    "            cumulative_clickmap = update_clickmap(cumulative_clickmap, pred_click, x_ab, threshold=threshold)\n",
    "            # new ColorNet input\n",
    "            colornet_input = torch.cat([x_L, cumulative_clickmap], dim=1)\n",
    "            pred_ab = colornet(colornet_input)\n",
    "            loss_iter_total += criterion(pred_ab, x_ab)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss_iter_total.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss_iter_total.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "def compute_psnr(pred, target):\n",
    "    \"\"\"\n",
    "    calculate single image PSNR, pred, target [2, H, W] [-1, 1]\n",
    "    \"\"\"\n",
    "    pred = (pred + 1) / 2  # [0,1]\n",
    "    target = (target + 1) / 2\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    psnr = 10 * torch.log10(1.0 / mse)\n",
    "    return psnr.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_psnr_iterative(colornet, editnet, test_loader, device, \n",
    "                            criterion=nn.MSELoss(), num_iterations=4, initial_clicks=50, threshold=0.5):\n",
    "    colornet.eval()\n",
    "    editnet.eval()\n",
    "    total_psnr = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for x_L, x_ab in test_loader:\n",
    "            x_L = x_L.to(device)   # [B,1,H,W]\n",
    "            x_ab = x_ab.to(device) # [B,2,H,W]\n",
    "            B, _, H, W = x_L.shape\n",
    "            cumulative_clickmap = initial_clickmap(x_L, x_ab, num_clicks=initial_clicks).to(device)  # [B,3,H,W]\n",
    "            \n",
    "            colornet_input = torch.cat([x_L, cumulative_clickmap], dim=1)\n",
    "            pred_ab = colornet(colornet_input)\n",
    "            \n",
    "            for i in range(1, num_iterations):\n",
    "                pred_click = editnet(pred_ab, x_L)  # [B,1,H,W]\n",
    "                cumulative_clickmap = update_clickmap(cumulative_clickmap, pred_click, x_ab, threshold=threshold)\n",
    "                colornet_input = torch.cat([x_L, cumulative_clickmap], dim=1)\n",
    "                pred_ab = colornet(colornet_input)\n",
    "            \n",
    "            for b in range(B):\n",
    "                psnr_val = compute_psnr(pred_ab[b], x_ab[b])\n",
    "                total_psnr += psnr_val\n",
    "            count += B\n",
    "    avg_psnr = total_psnr / count if count > 0 else 0\n",
    "    return avg_psnr\n",
    "\n",
    "def visualize_results(colornet, editnet, dataset, device, n_samples=3, clicks_list=[1, 2, 3], num_iterations=4, initial_clicks=50, threshold=0.5):\n",
    "    colornet.eval()\n",
    "    editnet.eval()\n",
    "    n_rows_per_sample = len(clicks_list)\n",
    "    n_cols = 3\n",
    "    total_rows = n_samples * n_rows_per_sample\n",
    "    fig, axes = plt.subplots(total_rows, n_cols, figsize=(15, 4 * n_samples), dpi=150)\n",
    "    if total_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    with torch.no_grad():\n",
    "        for sample_idx in range(n_samples):\n",
    "            idx = random.randint(0, len(dataset)-1)\n",
    "            x_L, x_ab = dataset[idx]   # x_L: [1,H,W], x_ab: [2,H,W]\n",
    "            x_L_batch = x_L.unsqueeze(0).to(device)\n",
    "            x_ab_batch = x_ab.unsqueeze(0).to(device)\n",
    "            x_L_np = x_L.cpu().numpy().squeeze()\n",
    "            # GT Lab\n",
    "            L_gt = x_L_np * 100.0\n",
    "            ab_gt = x_ab.cpu().numpy().squeeze().transpose(1,2,0) * 128.0\n",
    "            lab_gt = np.concatenate([L_gt[..., np.newaxis], ab_gt], axis=-1)\n",
    "            rgb_gt = lab2rgb(lab_gt.clip(np.array([0, -128, -128]), np.array([100, 127, 127])))\n",
    "            \n",
    "            for row_idx, n_clicks in enumerate(clicks_list):\n",
    "                cumulative_clickmap = initial_clickmap(x_L_batch, x_ab_batch, num_clicks=n_clicks).to(device)\n",
    "                pred_ab = None\n",
    "                colornet_input = torch.cat([x_L_batch, cumulative_clickmap], dim=1)\n",
    "                pred_ab = colornet(colornet_input)\n",
    "                for i in range(1, num_iterations):\n",
    "                    pred_click = editnet(pred_ab, x_L_batch)\n",
    "                    cumulative_clickmap = update_clickmap(cumulative_clickmap, pred_click, x_ab_batch, threshold=threshold)\n",
    "                    colornet_input = torch.cat([x_L_batch, cumulative_clickmap], dim=1)\n",
    "                    pred_ab = colornet(colornet_input)\n",
    "                    \n",
    "                pred_ab_np = pred_ab.squeeze().cpu().numpy() * 128.0  # [2,H,W]\n",
    "                L_pred = x_L_np * 100.0\n",
    "                lab_pred = np.concatenate([L_pred[np.newaxis, ...], pred_ab_np], axis=0)\n",
    "                lab_pred = lab_pred.transpose(1,2,0)\n",
    "                rgb_pred = lab2rgb(lab_pred.clip(np.array([0, -128, -128]), np.array([100, 127, 127])))\n",
    "                \n",
    "                global_row = sample_idx * n_rows_per_sample + row_idx\n",
    "                axes[global_row, 0].imshow(x_L_np, cmap='gray')\n",
    "                axes[global_row, 0].axis('off')\n",
    "                if row_idx == 0:\n",
    "                    axes[global_row, 0].set_title(f\"Sample {sample_idx+1}: Grayscale\")\n",
    "                axes[global_row, 1].imshow(rgb_pred)\n",
    "                axes[global_row, 1].axis('off')\n",
    "                if row_idx == 0:\n",
    "                    axes[global_row, 1].set_title(f\"Predicted with {n_clicks} clicks\")\n",
    "                else:\n",
    "                    axes[global_row, 1].set_title(f\"{n_clicks} clicks\")\n",
    "                axes[global_row, 2].imshow(rgb_gt)\n",
    "                axes[global_row, 2].axis('off')\n",
    "                if row_idx == 0:\n",
    "                    axes[global_row, 2].set_title(\"Ground Truth\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 valid images in data/coco/train2017\n",
      "Found 500 valid images in data/coco/val2017\n",
      "Found 500 images in validation set\n",
      "Epoch [1/16] - Loss: 0.1527\n",
      "Validation PSNR: 27.44 dB\n",
      "Saved best models!\n",
      "Epoch [2/16] - Loss: 0.0882\n",
      "Validation PSNR: 27.24 dB\n",
      "Epoch [3/16] - Loss: 0.0717\n",
      "Validation PSNR: 30.19 dB\n",
      "Saved best models!\n",
      "Epoch [4/16] - Loss: 0.0596\n",
      "Validation PSNR: 31.23 dB\n",
      "Saved best models!\n",
      "Epoch [5/16] - Loss: 0.0525\n",
      "Validation PSNR: 30.15 dB\n",
      "Epoch [6/16] - Loss: 0.0430\n",
      "Validation PSNR: 31.58 dB\n",
      "Saved best models!\n",
      "Epoch [7/16] - Loss: 0.0401\n",
      "Validation PSNR: 31.46 dB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 76\u001b[0m\n\u001b[0;32m     71\u001b[0m     visualize_results(colornet, editnet, dataset\u001b[38;5;241m=\u001b[39mtest_dataset, device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     72\u001b[0m                       n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, clicks_list\u001b[38;5;241m=\u001b[39mclicks_list, num_iterations\u001b[38;5;241m=\u001b[39mnum_iterations,\n\u001b[0;32m     73\u001b[0m                       initial_clicks\u001b[38;5;241m=\u001b[39minitial_clicks, threshold\u001b[38;5;241m=\u001b[39mthreshold)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m best_psnr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 32\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolornet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meditnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_clicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# scheduler_C.step()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# scheduler_E.step()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 79\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(colornet, editnet, dataloader, optimizer, device, criterion, num_iterations, initial_clicks, threshold)\u001b[0m\n\u001b[0;32m     76\u001b[0m loss_iter_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     78\u001b[0m colornet_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_L, cumulative_clickmap], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [B,4,H,W]\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m pred_ab \u001b[38;5;241m=\u001b[39m \u001b[43mcolornet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolornet_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m loss_iter_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(pred_ab, x_ab)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_iterations):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# EditNet input：predicted ab and clickmask\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 99\u001b[0m, in \u001b[0;36mUNetColorNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m btm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(d4_down)   \u001b[38;5;66;03m# [B,1024,H/16,W/16]\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# ------------------- Up -----------------------\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m u1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbtm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md4_skip\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# [B,512,H/8, W/8]\u001b[39;00m\n\u001b[0;32m    100\u001b[0m u2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(u1, d3_skip)       \u001b[38;5;66;03m# [B,256,H/4, W/4]\u001b[39;00m\n\u001b[0;32m    101\u001b[0m u3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(u2, d2_skip)       \u001b[38;5;66;03m# [B,128,H/2, W/2]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 48\u001b[0m, in \u001b[0;36mUp.forward\u001b[1;34m(self, x, skip)\u001b[0m\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_transpose(x)  \u001b[38;5;66;03m# upsampling => [B, out_channels, H*2, W*2]\u001b[39;00m\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([skip, x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# skip_channels + out_channels\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    lr = 1e-4\n",
    "    # lr_C = 1e-4  # ColorNet\n",
    "    # lr_E = 1e-4  # EditNet\n",
    "\n",
    "    batch_size = 2\n",
    "    num_epochs = 16\n",
    "    image_size = 128\n",
    "    num_iterations = 10\n",
    "    initial_clicks = 50\n",
    "    threshold = 0.7\n",
    "\n",
    "    colornet = UNetColorNet(in_channels=4, out_channels=2).to(device)\n",
    "    editnet = EditNet(in_channels=3, out_channels=1).to(device)\n",
    "    optimizer = optim.Adam(list(colornet.parameters()) + list(editnet.parameters()), lr=lr)\n",
    "    # optimizer_C = optim.Adam(colornet.parameters(), lr=lr_C)\n",
    "    # optimizer_E = optim.Adam(editnet.parameters(), lr=lr_E)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    # scheduler_C = optim.lr_scheduler.StepLR(optimizer_C, step_size=5, gamma=0.5)\n",
    "    # scheduler_E = optim.lr_scheduler.StepLR(optimizer_E, step_size=5, gamma=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_dataset = CocoColorizationDataset(root_dir=\"data/coco/train2017\", transform_size=image_size, limit=1000)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    test_dataset = CocoColorizationDataset(root_dir=\"data/coco/val2017\", transform_size=image_size, limit=500)\n",
    "    print(f\"Found {len(test_dataset)} images in validation set\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    best_psnr = -float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_one_epoch(colornet, editnet, dataloader, optimizer,\n",
    "                                                      device, criterion, num_iterations, initial_clicks, threshold)\n",
    "\n",
    "        scheduler.step()\n",
    "        # scheduler_C.step()\n",
    "        # scheduler_E.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}\")\n",
    "        current_psnr = evaluate_psnr_iterative(colornet, editnet, test_loader, device,\n",
    "                                               criterion, num_iterations=num_iterations,\n",
    "                                               initial_clicks=initial_clicks, threshold=threshold)\n",
    "        print(f\"Validation PSNR: {current_psnr:.2f} dB\")\n",
    "        if current_psnr > best_psnr:\n",
    "            best_psnr = current_psnr\n",
    "            torch.save({'colornet': colornet.state_dict(),\n",
    "                        'editnet': editnet.state_dict()}, \"best_models.pth\")\n",
    "            print(\"Saved best models!\")\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    clicks_list = [1, 5, 10]\n",
    "    _list = list(range(1, 11))\n",
    "    psnr_values = []\n",
    "    for n_clicks in _list:\n",
    "        avg_psnr = evaluate_psnr_iterative(colornet, editnet, test_loader, device, criterion,\n",
    "                                           num_iterations=num_iterations, initial_clicks=n_clicks, threshold=threshold)\n",
    "        psnr_values.append(avg_psnr)\n",
    "        print(f\"Initial Clicks = {n_clicks} | Avg PSNR = {avg_psnr:.2f} dB\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(_list, psnr_values, marker='o', linestyle='-')\n",
    "    plt.xlabel(\"Initial Clicks\")\n",
    "    plt.ylabel(\"PSNR (dB)\")\n",
    "    plt.title(\"PSNR vs. Initial Clicks\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n=== #Initial Clicks vs. PSNR ===\")\n",
    "    print(\"Clicks\\tPSNR(dB)\")\n",
    "    for i in range(len(clicks_list)):\n",
    "        print(f\"{clicks_list[i]}\\t{psnr_values[i]:.2f}\")\n",
    "        \n",
    "    visualize_results(colornet, editnet, dataset=test_dataset, device=device,\n",
    "                      n_samples=3, clicks_list=clicks_list, num_iterations=num_iterations,\n",
    "                      initial_clicks=initial_clicks, threshold=threshold)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
